{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "if 'init_done' in globals():\n",
    "    matplotlib.use(\"pgf\")\n",
    "    matplotlib.rcParams.update({\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        'font.family': 'serif',\n",
    "        'text.usetex': True,\n",
    "        'pgf.rcfonts': False,\n",
    "    })\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "\n",
    "init_done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "WINDOW_LENGTH = 16\n",
    "FOLDER_SUFFIX = ''\n",
    "LABEL_NAME = 'label_death_continuous' # Possible values: 'label_death_icu', 'label_death_continuous'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load result files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_train = {'cml':{}, 'fl':{}, 'lml':{}}\n",
    "scores_valid = {'cml':{}, 'fl':{}, 'lml':{}}\n",
    "scores_test =  {'cml':{}, 'fl':{}, 'lml':{}}\n",
    "predictions =  {'cml':{}, 'fl':{}, 'lml':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(n_clients, fl=False):\n",
    "    l = 'cont' if LABEL_NAME == 'label_death_continuous' else 'bin'\n",
    "\n",
    "    path =  f'./scores/{l:s}_{WINDOW_LENGTH:d}h{FOLDER_SUFFIX:s}/'\n",
    "    path += ('scores_fl_' if fl else 'scores_')\n",
    "    path += f'{l:s}_{n_clients:d}clients_{WINDOW_LENGTH:d}h.pickle'\n",
    "\n",
    "    print(f'Loading file \"{path:s}\"', end='...')\n",
    "\n",
    "    key = 'cml' if n_clients == 1 else 'fl' if fl else 'lml'\n",
    "\n",
    "    try:\n",
    "        with open(path, 'rb') as file:\n",
    "            scores_train[key][n_clients], scores_valid[key][n_clients], scores_test[key][n_clients], predictions[key][n_clients] = pickle.load(file)\n",
    "    except:\n",
    "        with open(path, 'rb') as file:\n",
    "            scores_train[key][n_clients], scores_valid[key][n_clients], scores_test[key][n_clients] = pickle.load(file)\n",
    "\n",
    "    print(f'Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load central scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(n_clients=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FL-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 clients\n",
    "load(n_clients=2, fl=True)\n",
    "\n",
    "# 4 clients\n",
    "load(n_clients=4, fl=True)\n",
    "\n",
    "# 8 clients\n",
    "#load(n_clients=8, fl=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load local scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 clients\n",
    "load(n_clients=2)\n",
    "\n",
    "# 4 clients\n",
    "load(n_clients=4)\n",
    "\n",
    "# 8 clients\n",
    "#load(n_clients=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate scores with sk-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import enumerate_predictions\n",
    "n_labels = 5 if LABEL_NAME == 'label_death_continuous' else 2\n",
    "n_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in predictions:\n",
    "    for n_clients in predictions[model]:\n",
    "        # Init scores-arrays:\n",
    "        scores_test[model][n_clients]['AUROC'] = np.zeros((n_fold, n_labels))\n",
    "        scores_test[model][n_clients]['AUPRC'] = np.zeros((n_fold, n_labels))\n",
    "        scores_test[model][n_clients]['precision'] = np.zeros((n_fold, n_labels))\n",
    "        scores_test[model][n_clients]['recall'] = np.zeros((n_fold, n_labels))\n",
    "        scores_test[model][n_clients]['MAE'] = np.zeros(n_fold)\n",
    "        scores_test[model][n_clients]['MSE'] = np.zeros(n_fold)\n",
    "        \n",
    "        # Calculate actual number of scores:\n",
    "        n = 1 if model=='fl' else n_clients\n",
    "        \n",
    "        for fold in range(n_fold):\n",
    "            f = 1. / float(n) \n",
    "            for i in range(n):\n",
    "                y_true = []\n",
    "                y_pred = []\n",
    "                for t, p in enumerate_predictions(predictions[model][n_clients], n_labels=n_labels, client=i, fold=fold):\n",
    "                    y_true.append(t.astype(int))\n",
    "                    y_pred.append(p.astype(float))\n",
    "                y_true = np.array(y_true)\n",
    "                y_pred = np.array(y_pred)\n",
    "\n",
    "                # Calculate classification metrics:\n",
    "                for label in range(n_labels):\n",
    "                    prc_crv, rcl_crv, _ = sklearn.metrics.precision_recall_curve(y_true[:, label], y_pred[:, label])\n",
    "\n",
    "                    scores_test[model][n_clients]['AUROC'][fold, label] += f * sklearn.metrics.roc_auc_score(y_true[:, label], y_pred[:, label])\n",
    "                    scores_test[model][n_clients]['AUPRC'][fold, label] += f * sklearn.metrics.auc(rcl_crv, prc_crv)\n",
    "                    scores_test[model][n_clients]['precision'][fold, label] += f * sklearn.metrics.precision_score(y_true[:, label], np.round(y_pred[:, label]))\n",
    "                    scores_test[model][n_clients]['recall'][fold, label] += f * sklearn.metrics.recall_score(y_true[:, label], np.round(y_pred[:, label]))\n",
    "\n",
    "                # Calculate error scores:\n",
    "                y = predictions[model][n_clients][fold, i]\n",
    "                y = y[~np.isnan(y).any(axis=1), :]\n",
    "                scores_test[model][n_clients]['MAE'][fold] += f * sklearn.metrics.mean_absolute_error(y[:, 0], y[:, 1])\n",
    "                scores_test[model][n_clients]['MSE'][fold] += f * sklearn.metrics.mean_squared_error(y[:, 0], y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate F1-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_f1(scores):\n",
    "    precision = scores['precision']\n",
    "    recall = scores['recall']\n",
    "\n",
    "    scores['F1'] = 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in scores_train:\n",
    "    for n_clients in scores_train[model]:\n",
    "        add_f1(scores_train[model][n_clients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in scores_valid:\n",
    "    for n_clients in scores_valid[model]:\n",
    "        add_f1(scores_valid[model][n_clients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in scores_test:\n",
    "    for n_clients in scores_test[model]:\n",
    "        add_f1(scores_test[model][n_clients])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create latex table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['AUROC', 'AUPRC', 'F1', 'precision', 'recall']\n",
    "errors = ['MAE', 'MSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = ''\n",
    "\n",
    "for m in metrics:\n",
    "    # Row title:\n",
    "    table += f'\\\\head\\u007B{m:s}\\u007D\\t'\n",
    "\n",
    "    # Central model score:\n",
    "    avg = np.nanmean(scores_test['cml'][1][m][:,1:])\n",
    "    std = np.nanstd(scores_test['cml'][1][m][:,1:])\n",
    "    table += f'& ${avg:.2f} \\\\pm {std:.2f}$\\t'\n",
    "\n",
    "    # FL and local model scores:\n",
    "    for model in ['fl', 'lml']:\n",
    "        for n_clients in [2, 4]:\n",
    "        #for n_clients in [2, 4, 8]:\n",
    "            avg = np.nanmean(scores_test[model][n_clients][m][:,1:])\n",
    "            std = np.nanstd(scores_test[model][n_clients][m][:,1:])\n",
    "            table += f'& ${avg:.2f} \\\\pm {std:.2f}$\\t'\n",
    "\n",
    "    table += '\\\\\\\\\\n'\n",
    "\n",
    "for e in errors:\n",
    "    # Row title:\n",
    "    table += f'\\\\head\\u007B{e:s}\\u007D\\t'\n",
    "\n",
    "    # Central model score:\n",
    "    avg = np.nanmean(scores_test['cml'][1][e])\n",
    "    std = np.nanstd(scores_test['cml'][1][e])\n",
    "    table += f'& ${avg:.2f} \\\\pm {std:.2f}$\\t'\n",
    "\n",
    "    # FL and local model scores:\n",
    "    for model in ['fl', 'lml']:\n",
    "        for n_clients in [2, 4]:\n",
    "        #for n_clients in [2, 4, 8]:\n",
    "            avg = np.nanmean(scores_test[model][n_clients][e])\n",
    "            std = np.nanstd(scores_test[model][n_clients][e])\n",
    "            table += f'& ${avg:.2f} \\\\pm {std:.2f}$\\t'\n",
    "\n",
    "    table += '\\\\\\\\\\n'\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_curve(scores, metric, color, label, ax, client=None, round=None, step=1, averaging='mean'):\n",
    "    # calculate y-values:\n",
    "    y = scores[metric]\n",
    "    if client != None:\n",
    "        y = y[round, client] if round != None else y[:, client]\n",
    "\n",
    "    elif round != None:\n",
    "        y = y[round, :]\n",
    "\n",
    "    y_min = y\n",
    "    y_avg = y\n",
    "    y_max = y\n",
    "    while len(y_avg.shape) > 1:\n",
    "        y_min = np.min(y_min, axis=0)\n",
    "        y_avg = np.mean(y_avg, axis=0)\n",
    "        y_max = np.max(y_max, axis=0)\n",
    "\n",
    "    # calculate number of values:\n",
    "    n = min(y_avg.shape[0], int(50/step))\n",
    "    while np.isnan(y_avg[n-1]):\n",
    "        n -=1\n",
    "\n",
    "    # calculate x-values:\n",
    "    x = np.arange(1,(n*step)+1,step)\n",
    "    \n",
    "    # plot curve:\n",
    "    if averaging == 'min':\n",
    "        ax.plot(x[:n], y_min[:n], color=color, label=label)\n",
    "\n",
    "    elif averaging == 'max':\n",
    "        ax.plot(x[:n], y_max[:n], color=color, label=label)\n",
    "\n",
    "    else:\n",
    "        ax.plot(x[:n], y_avg[:n], color=color, label=label)\n",
    "\n",
    "    return n*step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploratory_plot(metric, n_clients=1, fl=False, averaging='mean'):\n",
    "    n_rounds = 5\n",
    "\n",
    "    fig = plt.figure(figsize=(2*n_rounds, 1.5*n_clients))\n",
    "\n",
    "    for i in range(n_rounds):\n",
    "        for j in range(n_clients):\n",
    "            ax = fig.add_subplot(n_clients, n_rounds, j*n_rounds + i + 1)\n",
    "            n = 0\n",
    "\n",
    "            # Print curves:\n",
    "            if n_clients==1:\n",
    "                n = max(n, add_curve(scores_train['cml'][1], metric, '#3465a4', 'train', ax, round=i, averaging=averaging))\n",
    "                n = max(n, add_curve(scores_valid['cml'][1], metric, '#f37500', 'valid', ax, round=i, averaging=averaging))\n",
    "            \n",
    "            elif fl:\n",
    "                n = max(n, add_curve(scores_train['fl'][n_clients], metric, '#3465a4', 'train', ax, client=j, round=i, averaging=averaging))\n",
    "                n = max(n, add_curve(scores_valid['fl'][n_clients], metric, '#f37500', 'valid', ax, client=j, round=i, averaging=averaging))\n",
    "                \n",
    "            else:\n",
    "                n = max(n, add_curve(scores_train['lml'][n_clients], metric, '#3465a4', 'train', ax, client=j, round=i, averaging=averaging))\n",
    "                n = max(n, add_curve(scores_valid['lml'][n_clients], metric, '#f37500', 'valid', ax, client=j, round=i, averaging=averaging))\n",
    "\n",
    "            # Print best weights:\n",
    "            if n < 50:\n",
    "                plt.axvline(x=n-20, color='#f10d0c', linestyle='--', label='best')\n",
    "\n",
    "            ax.set_xticks(np.arange(n+1, step=10))\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(metric)\n",
    "\n",
    "            if j == 0:\n",
    "                ax.set_title(f'CV-iteration {i+1:d}')\n",
    "                \n",
    "            elif j == n_clients-1:\n",
    "                ax.set_xlabel('fl-round' if fl else 'epoch')\n",
    "\n",
    "            if i == n_rounds-1 and j == n_clients-1:\n",
    "                ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_plot('loss', n_clients=4, fl=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_plot(metric, ax, n_clients=1, client=None, round=None, fl=False, step=1, legend=True, averaging='mean'):\n",
    "    n = 0\n",
    "\n",
    "    # Print curves:\n",
    "    if n_clients==1:\n",
    "        n = max(n, add_curve(scores_train['cml'][1], metric, '#3465a4', 'training', ax, round=round, averaging=averaging))\n",
    "        n = max(n, add_curve(scores_valid['cml'][1], metric, '#f37500', 'validation', ax, round=round, averaging=averaging))\n",
    "    \n",
    "    elif fl:\n",
    "        n = max(n, add_curve(scores_train['fl'][n_clients], metric, '#3465a4', 'training', ax, client=client, round=round, averaging=averaging))\n",
    "        n = max(n, add_curve(scores_valid['fl'][n_clients], metric, '#f37500', 'validation', ax, client=client, round=round, averaging=averaging))\n",
    "        \n",
    "    else:\n",
    "        n = max(n, add_curve(scores_train['lml'][n_clients], metric, '#3465a4', 'training', ax, client=client, round=round, averaging=averaging))\n",
    "        n = max(n, add_curve(scores_valid['lml'][n_clients], metric, '#f37500', 'validation', ax, client=client, round=round, averaging=averaging))\n",
    "\n",
    "    # Print best weights:\n",
    "    if n < 50:\n",
    "        plt.axvline(x=n-20, color='#f10d0c', linestyle='--', label='best weights')\n",
    "\n",
    "    ax.set_xticks(np.arange(n+1, step=step))\n",
    "\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel('FL-round' if fl else 'epoch')\n",
    "    if legend: ax.legend() \n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = None\n",
    "round = 2\n",
    "fl = True\n",
    "n_clients=4\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "learning_plot('loss', fig.add_subplot(2, 1, 1), step=2, client=client, round=round, fl=fl, n_clients=n_clients, legend=False)\n",
    "\n",
    "learning_plot('precision', fig.add_subplot(2, 2, 3), step=5, client=client, round=round, fl=fl, n_clients=n_clients, legend=False)\n",
    "learning_plot('recall', fig.add_subplot(2, 2, 4), step=5, client=client, round=round, fl=fl, n_clients=n_clients)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('./pictures/learning_curve.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = None\n",
    "round = 3\n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "learning_plot('loss', ax, step=10, client=client, round=round, fl=False, n_clients=1, legend=False)\n",
    "ax.set_title('loss CML')\n",
    "#ax.set_ylim(.0,.8)\n",
    "#ax.set_yticks([.0,.2,.4,.6,.8])\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "learning_plot('loss', ax, step=10, client=client, round=round, fl=True, n_clients=4, legend=True)\n",
    "ax.set_title('loss FL 4 clients')\n",
    "#ax.set_ylim(.0,.8)\n",
    "#ax.set_yticks([.0,.2,.4,.6,.8])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "#fig.savefig('./pictures/learning_curve_cont.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores / Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cml = True\n",
    "plot_fl_clients = [4]\n",
    "plot_lml_clients = [4]\n",
    "\n",
    "plot_train = False\n",
    "plot_valid = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(metric, ax, client=None, round=None, step=1, legend=True, y_ticks=[]):\n",
    "    n = 0\n",
    "    colors = [\n",
    "        '#f37500', #(234, 117,   0)\n",
    "        '#069a2E', #(  6, 154,  46)\n",
    "        '#3465a4', #( 52, 101, 164)\n",
    "        '#780373', #(120,   3, 115)\n",
    "        '#f10d0c'  #(241,  13,  12)\n",
    "    ]\n",
    "\n",
    "    # Plot curves:\n",
    "    if plot_cml:\n",
    "        if plot_train: n = max(n, add_curve(scores_train['cml'][1], metric, colors.pop(), 'CML train.', ax, round=round))\n",
    "        if plot_valid: n = max(n, add_curve(scores_valid['cml'][1], metric, colors.pop(), 'CML valid.', ax, round=round))\n",
    "\n",
    "    for n_clients in plot_fl_clients:\n",
    "        if plot_train: n = max(n, add_curve(scores_train['fl'][n_clients], metric, colors.pop(), f'FL train. ({n_clients:d} cl.)', ax, client=client, round=round, step=1))\n",
    "        if plot_valid: n = max(n, add_curve(scores_valid['fl'][n_clients], metric, colors.pop(), f'FL valid. ({n_clients:d} cl.)', ax, client=client, round=round, step=1))\n",
    "        \n",
    "    for n_clients in plot_lml_clients:\n",
    "        if plot_train: n = max(n, add_curve(scores_train['lml'][n_clients], metric, colors.pop(), f'LML train. ({n_clients:d} cl.)', ax, client=client, round=round))\n",
    "        if plot_valid: n = max(n, add_curve(scores_valid['lml'][n_clients], metric, colors.pop(), f'LML valid. ({n_clients:d} cl.)', ax, client=client, round=round))\n",
    "\n",
    "    ax.set_xticks(np.arange(n+1, step=step))\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_title(metric)\n",
    "    if len(y_ticks) > 0: ax.set_yticks(y_ticks)\n",
    "    if legend: ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = None\n",
    "round = 2\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "plot('AUROC', fig.add_subplot(1, 3, 1), step=10, y_ticks=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0], client=client, round=round)\n",
    "plot('AUPRC', fig.add_subplot(1, 3, 2), step=10, y_ticks=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], client=client, round=round, legend=False)\n",
    "plot('F1',    fig.add_subplot(1, 3, 3), step=10, y_ticks=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], client=client, round=round, legend=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('./pictures/score_curves.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = None\n",
    "round = 1\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "plot('MAE', fig.add_subplot(1, 2, 1), step=10, y_ticks=[.0, .1, .2, .3, .4, .5], client=client, round=round, legend=False)\n",
    "plot('MSE', fig.add_subplot(1, 2, 2), step=10, y_ticks=[.0, .1, .2, .3, .4, .5], client=client, round=round)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('./pictures/error_curves.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "y_test_cml = pd.DataFrame(predictions['cml'][1][0,0,:,:])\n",
    "y_test_fl2 = pd.DataFrame(predictions['fl'][2][0,0,:,:]).dropna()\n",
    "y_test_fl4 = pd.DataFrame(predictions['fl'][4][0,0,:,:]).dropna()\n",
    "#y_test_fl8 = pd.DataFrame(predictions['fl'][8][0,0,:,:]).dropna()\n",
    "y_test_lml2 = pd.DataFrame(predictions['lml'][2][0,0,:,:]).dropna()\n",
    "y_test_lml4 = pd.DataFrame(predictions['lml'][4][0,0,:,:]).dropna()\n",
    "#y_test_lml8 = pd.DataFrame(predictions['lml'][8][0,0,:,:]).dropna()\n",
    "for f in range(1,5):\n",
    "    y_test_cml = y_test_cml.append(pd.DataFrame(predictions['cml'][1][f,0,:,:])).dropna()\n",
    "    y_test_fl2 = y_test_fl2.append(pd.DataFrame(predictions['fl'][2][f,0,:,:])).dropna()\n",
    "    y_test_fl4 = y_test_fl4.append(pd.DataFrame(predictions['fl'][4][f,0,:,:])).dropna()\n",
    "    #y_test_fl8 = y_test_fl8.append(pd.DataFrame(predictions['fl'][8][f,0,:,:])).dropna()\n",
    "    y_test_lml2 = y_test_lml2.append(pd.DataFrame(predictions['lml'][2][f,0,:,:])).dropna()\n",
    "    y_test_lml4 = y_test_lml4.append(pd.DataFrame(predictions['lml'][4][f,0,:,:])).dropna()\n",
    "    #y_test_lml8 = y_test_lml8.append(pd.DataFrame(predictions['lml'][8][f,0,:,:])).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_fl4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    '#f37500', #(234, 117,   0)\n",
    "    '#069a2E', #(  6, 154,  46)\n",
    "    '#3465a4', #( 52, 101, 164)\n",
    "    '#780373', #(120,   3, 115)\n",
    "    '#f10d0c'  #(241,  13,  12)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(model, n_clients, labels, predictions, ax, **kwargs):\n",
    "    labels = labels.round(decimals=0, out=None).astype(int)\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "    #auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
    "    auc = np.nanmean(scores_test[model][n_clients]['AUROC'][:,1:])\n",
    "    score = model.upper()\n",
    "    if n_clients > 1:\n",
    "        score += ' %d cl.' % (n_clients)\n",
    "    score += ' AUC=%.2f' % (auc)\n",
    "    ax.plot(fp, tp, label=score,  **kwargs)\n",
    "    ax.set_xlabel('False Positive Rate (FPR)')\n",
    "    ax.set_xlim(-.05, 1.05)\n",
    "    ax.set_ylabel('True Positive Rate (TPR)')\n",
    "    ax.set_ylim(-.05, 1.05)\n",
    "    ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    ax.grid(True)\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prc(model, n_clients, labels, predictions, ax, **kwargs):\n",
    "    labels = labels.round(decimals=0, out=None).astype(int)\n",
    "    precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)\n",
    "    #auc = sklearn.metrics.auc(recall, precision)\n",
    "    auc = np.nanmean(scores_test[model][n_clients]['AUPRC'][:,1:])\n",
    "    score = model.upper()\n",
    "    if n_clients > 1:\n",
    "        score += ' %d cl.' % (n_clients)\n",
    "    score += ' AUC=%.2f' % (auc)\n",
    "    ax.plot(recall, precision, label=score, linewidth=2, **kwargs)\n",
    "    ax.set_title('Precision-Recall (PR) Curve')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_xlim(-.05, 1.05)\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_ylim(-.05, 1.05)\n",
    "    ax.grid(True)\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "# plot AUROC:\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "\n",
    "plot_roc('cml', 1, y_test_cml[0], y_test_cml[1], ax, color=colors[0])\n",
    "plot_roc('fl',  2, y_test_fl2[0], y_test_fl2[1], ax, color=colors[1])\n",
    "plot_roc('fl',  4, y_test_fl4[0], y_test_fl4[1], ax, color=colors[2])\n",
    "#plot_roc('fl',  8, y_test_fl8[0], y_test_fl8[1], ax, color=colors[2])\n",
    "plot_roc('lml',  2, y_test_lml2[0], y_test_lml2[1], ax, color=colors[3])\n",
    "plot_roc('lml',  4, y_test_lml4[0], y_test_lml4[1], ax, color=colors[4])\n",
    "#plot_roc('lml',  8, y_test_lml8[0], y_test_lml8[1], ax, color=colors[4])\n",
    "\n",
    "no_skill = len(y_test_cml[0][y_test_cml[0]==1]) / len(y_test_cml[0])\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', label='baseline')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# plot AUPRC:\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "\n",
    "plot_prc('cml', 1, y_test_cml[0], y_test_cml[1], ax, color=colors[0])\n",
    "plot_prc('fl',  2, y_test_fl2[0], y_test_fl2[1], ax, color=colors[1])\n",
    "plot_prc('fl',  4, y_test_fl4[0], y_test_fl4[1], ax, color=colors[2])\n",
    "#plot_prc('fl',  8, y_test_fl8[0], y_test_fl8[1], ax, color=colors[2])\n",
    "plot_prc('lml',  2, y_test_lml2[0], y_test_lml2[1], ax, color=colors[3])\n",
    "plot_prc('lml',  4, y_test_lml4[0], y_test_lml4[1], ax, color=colors[4])\n",
    "#plot_prc('lml',  8, y_test_lml8[0], y_test_lml8[1], ax, color=colors[4])\n",
    "\n",
    "no_skill = len(y_test_cml[0][y_test_cml[0]==1]) / len(y_test_cml[0])\n",
    "ax.plot([0, 1], [no_skill, no_skill], linestyle='--', label='baseline')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# save plot:\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "#fig.savefig('./pictures/auc_curves.pdf')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09804e24ad6773f4299ff941abdb533da0618f58a933eb5ec00c0e9780539224"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
